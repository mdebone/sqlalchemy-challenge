# sqlalchemy-challenge
So this was interesting, for whatever reason I got it in my head that first was the most recent value, which meant that I was sidetracked on pulling dates from 1/1/2010 - 365 days. You got to laugh, but I learned a valuable lesson that first equates to first data point, which is actually 'last' on longitudinal studies. Life lesson learned.

From there, when I called the first 15 measurement dates, there were 'None' values, but given the prevalence of '0.0' throughout the data, I just thought that these were incorrectly entered but nonetheless represented valuable data, so I thought that I would have to change the 'None' values  to 0.0, prior to getting anything accomplished in regards to analysis. 
After a few permutations and filter, filter_by and the like, I realized that these are propably NaN values and I should have been trying to remove them that way, also that if I wasnted to change the values there is no mass function, from what I can determine, that would allow me to call the none values and replace them with '0.0', I would have to do it line by line using update.

From there my next kurfuffel was to assume that because the dataset began at 01-01-2010, these were annual tables, so if the last year was 2017, then all I needed to do was >2016-12-31 and <= 2017-12-31. But, as Im sure one has guessed by now, the last 12 months of data does not represent a calendar year, the year ends on 08-23, so back to the beginning all over again, learned the value of "order_by(whatever.date.desc()).first()" the hard way but a lesson that I am not soon to forget.

Somewhere along the line, I reset the jupyter notebook and recalled the base.keys, which resulted in [] for the output and along down the line you can't call a class anymore, and you cant call a table anymore, and you can't call a column anymore because each one of those is dependent on the inital call. I looked into how to revert, I looked into how to reset, I opened new jupyter notebooks, I changed the path to the resources .sqlite file, and nothing worked. You have to delete the first file and redownload the inital file beause even if you change table names and the like the values are still stored in there, you can't session close, you can't really drop all, although I think I might have got that wrong, you have to delete the file and redownload it.
Thats another life lesson, but hey, lesson learned. 

I also got into a bit of a pinch when I had it in my head that declarative base was somehow superior to automap base, and hijinks insued. This is by no means complete, I wasn't able to get to the second part of the homework with the Climate App, so I will have to revist this as soon as I get a few hours sleep. Also, I am aware that for whatever reason my graph is wrong, I don't think that it is pulling all of the stations, just by looking at the output presented in the homework file on GitLab and their graph.
But I did get the second part looking at the stations is actually looking at the Measurement table, although I was unable to group them in descending order for whatever reason.
